notes

check that topology of net is of type B
check that we can reload cunn conv layer in a nn conv layer (convert to float ?)

void torch class on loading of model
are we using the same version of Lua ?


model loads on GPU
model conversion fails with checkpoint attributes null, but printing checkpoint shows real structure
--> let us try protos=torch.load

now extend image to 4D tensor in orbit

find imagenet 1000 lines file
always the same output even if input tensor is 0

with GPU
/home/yves/torch/install/share/lua/5.1/nn/Container.lua:67: 
In 1 module of nn.Sequential:

the model does not include the ouput layer (cross entropy for training, softmax for prediction)

LSTM

input : 
- 1 image, 5 captions
- text vocabulary 8801 then word2vec 1000
- cont_sentence a boolean
- score the top level resnet layer

- add a BOS and an EOS to word2vec target values ?
- what is the memory size of the LSTM ?


then :
2 layer LSTM 1000 rho=16
output :
	output innerProduct 1000 -> 8801
	softmax
==============
using GloVe 300
- datasize 800M is big
- filtering of punctuation, numbers and common words is not done

check UNK value in word2vec
check words sent to word2vec, they should not repeat
correct spelling of text files
optim.Logger and plots

compute word from vec in caption model ? differentiable ?
input of LSTM must be a tensor not a table of tensors ? should we use parallellTable ?


--- 16/05/2017 ---
lstm out is random
try main.lua in LSTM PREDICT mode
Must retrain to save lstm as :float().
Record target sentence as analyzed, using glove:distance(), to check that learning goes well.
This should match the filename/target file.
------------------
20/05
next print tensors in file and see why they are so close for mse yet so different for words see disputils

multiply input by sqrt(vector_size) to avoid using tanh in lstm in linear regime

scaling triggers nan in loss

try tiny dataset of 1 or 10 samples

--- 01/06
LRCN 2f is better -> adapt NN architecture -> we should add the image representation on LSTM layer 2
check if there is a need for a non linear output stage -> replace glove by a linear layer before and after all LSTM, mapping dictionnary one hot vectors to glove embedding, using the vector text file to initialize the parameters
nn.Linear

-- 02/06
we must define a multiple entry model :
- input sentence goes to the linear input
- input image goes to the lstm entry of layer 2
so forward is not as simple a a single input network

we need a way to block the weights of a module to be update by BP : subclass the layer and overrid the parameterupdate
-->
You can set the learning rate of certain layers to zero by overriding their updateParameters and accGradParameters to zero. You don't necessarily have to subclass, but it is cleaner.
https://gist.github.com/soumith/6cd0f9b8462d0507a91b
https://www.reddit.com/r/MachineLearning/comments/44ochv/forcing_learning_rate_to_zero_in_torch/

also we need a way to make connection between non adjacent layers --> nngraph


check representation of words after the end of sentence, remove point ?
coco_to_hdf5.py vocabulary detection is limited to words which appear in at least 5 sentences
new architecture : Glove -> 1000 , image = first input instead of BOS
so the input Linear is not part of the caption.lua network

last word overwrites all others in target

try learning rate as 1/x not an exponential

getparams ?
two level lstm used : beware of camptibility ...
now I save the full lstm, not just the flattended parameters used for propagation

convert model to float() before save
the saved lstm uses a sequencer
the mac install is broken there is a conflict between nn and rnn, see rnn install recommandations
lstm.module.module:get(1) gives first lstm

check generated sentence : lstm ouputs only unk whichis the last vector coordinate to 1
glove envoding is not using the good files, size is incorrect
see test.lua

------
Process
train resnet on imagenet, remove last layer, and keep a 1000x1 output vector size
train glove on train caption of coco, keeping all words
preprocess coco dataset to create coco-caption.txt which holds file names and sentences for train, val datasets
train lstm uging GloVe embeddings and resnet as input
predict using predict.lua which is an orbit webapp
-------

halve batch size : crash after the same number of batches (625) , so half the number of samples ??

