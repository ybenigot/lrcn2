notes

check that topology of net is of type B
check that we can reload cunn conv layer in a nn conv layer (convert to float ?)

void torch class on loading of model
are we using the same version of Lua ?


model loads on GPU
model conversion fails with checkpoint attributes null, but printing checkpoint shows real structure
--> let us try protos=torch.load

now extend image to 4D tensor in orbit

find imagenet 1000 lines file
always the same output even if input tensor is 0

with GPU
/home/yves/torch/install/share/lua/5.1/nn/Container.lua:67: 
In 1 module of nn.Sequential:

the model does not include the ouput layer (cross entropy for training, softmax for prediction)

LSTM

input : 
- 1 image, 5 captions
- text vocabulary 8801 then word2vec 1000
- cont_sentence a boolean
- score the top level resnet layer

- add a BOS and an EOS to word2vec target values ?
- what is the memory size of the LSTM ?


then :
2 layer LSTM 1000 rho=16
output :
	output innerProduct 1000 -> 8801
	softmax
==============
using GloVe 300
- datasize 800M is big
- filtering of punctuation, numbers and common words is not done

check UNK value in word2vec
check words sent to word2vec, they should not repeat
correct spelling of text files
optim.Logger and plots

compute word from vec in caption model ? differentiable ?
input of LSTM must be a tensor not a table of tensors ? should we use parallellTable ?


--- 16/05/2017 ---
lstm out is random
try main.lua in LSTM PREDICT mode
Must retrain to save lstm as :float().
Record target sentence as analyzed, using glove:distance(), to check that learning goes well.
This should match the filename/target file.
------------------
20/05
next print tensors in file and see why they are so close for mse yet so different for words see disputils

multiply input by sqrt(vector_size) to avoid using tanh in lstm in linear regime

scaling triggers nan in loss

try tiny dataset of 1 or 10 samples

--- 01/06
LRCN 2f is better -> adapt NN architecture -> we should add the image representation on LSTM layer 2
check if there is a need for a non linear output stage -> replace glove by a linear layer before and after all LSTM, mapping dictionnary one hot vectors to glove embedding, using the vector text file to initialize the parameters
nn.Linear


